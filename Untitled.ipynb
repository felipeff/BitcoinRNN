{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data from CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            BTC-USD_close  BTC-USD_volume  LTC-USD_close  LTC-USD_volume  \\\n",
      "time                                                                       \n",
      "1528968660    6489.549805        0.587100      96.580002        9.647200   \n",
      "1528968720    6487.379883        7.706374      96.660004      314.387024   \n",
      "1528968780    6479.410156        3.088252      96.570000       77.129799   \n",
      "1528968840    6479.410156        1.404100      96.500000        7.216067   \n",
      "1528968900    6479.979980        0.753000      96.389999      524.539978   \n",
      "\n",
      "            ETH-USD_close  ETH-USD_volume  BCH-USD_close  BCH-USD_volume  \n",
      "time                                                                      \n",
      "1528968660            NaN             NaN     871.719971        5.675361  \n",
      "1528968720      486.01001       26.019083     870.859985       26.856577  \n",
      "1528968780      486.00000        8.449400     870.099976        1.124300  \n",
      "1528968840      485.75000       26.994646     870.789978        1.749862  \n",
      "1528968900      486.00000       77.355759     870.000000        1.680500  \n"
     ]
    }
   ],
   "source": [
    "main_df = pd.DataFrame()\n",
    "\n",
    "# CSV Files \n",
    "ratios = [\"BTC-USD\", \"LTC-USD\", \"ETH-USD\", \"BCH-USD\"]\n",
    "\n",
    "for ratio in ratios:\n",
    "    dataset = f'crypto_data/{ratio}.csv'\n",
    "    \n",
    "    df = pd.read_csv(dataset,\n",
    "                names=['time', 'low',' high', 'open', 'close', 'volume']\n",
    "                )\n",
    "    df.rename(columns={\"close\":f\"{ratio}_close\", \"volume\":f\"{ratio}_volume\"}, inplace=True)\n",
    "    \n",
    "    df.set_index(\"time\", inplace=True)\n",
    "    \n",
    "    # get rid of the open/high/low\n",
    "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]\n",
    "    \n",
    "    if len(main_df) == 0:\n",
    "        main_df = df\n",
    "    else:\n",
    "        main_df = main_df.join(df)\n",
    "print(main_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the sequential data, but we need the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 60 min to predict the next 3 minutes of the LTC-USD\n",
    "SEQ_LEN = 60\n",
    "FUTURE_PERIOD_PREDICT = 3\n",
    "RATIO_TO_PREDICT = \"LTC-USD\"\n",
    "\n",
    "def classify(current, future):\n",
    "    # if the price in the future is greater than the current then we return 1. else 0\n",
    "    # we will teach our RNN model that 1 is good\n",
    "    if float(future) > float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            LTC-USD_close     future\n",
      "time                                \n",
      "1528968660      96.580002  96.500000\n",
      "1528968720      96.660004  96.389999\n",
      "1528968780      96.570000  96.519997\n",
      "1528968840      96.500000  96.440002\n",
      "1528968900      96.389999  96.470001\n"
     ]
    }
   ],
   "source": [
    "# Let's get the future price for the asset that we want to predict and add that to a FUTURE column\n",
    "main_df['future'] = main_df[f\"{RATIO_TO_PREDICT}_close\"].shift(-FUTURE_PERIOD_PREDICT)\n",
    "\n",
    "print(main_df[[f\"{RATIO_TO_PREDICT}_close\", \"future\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            LTC-USD_close     future  target\n",
      "time                                        \n",
      "1528968660      96.580002  96.500000       0\n",
      "1528968720      96.660004  96.389999       0\n",
      "1528968780      96.570000  96.519997       0\n",
      "1528968840      96.500000  96.440002       0\n",
      "1528968900      96.389999  96.470001       1\n"
     ]
    }
   ],
   "source": [
    "# Now that we know the current price and the future price\n",
    "# We can apply our classify function to create a Target column\n",
    "# This column will show 0 if the future price is less than the current price and 1 if the future price is better than the current price\n",
    "main_df['target'] = list(map(classify, main_df[f\"{RATIO_TO_PREDICT}_close\"], main_df[\"future\"]))\n",
    "\n",
    "print(main_df[[f\"{RATIO_TO_PREDICT}_close\", \"future\", \"target\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate our training data from the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = sorted(main_df.index.values)\n",
    "# 5% for validation and 95% for training\n",
    "last_5pct = times[-int(0.05*len(times))]\n",
    "\n",
    "validation_main_df = main_df[(main_df.index >= last_5pct)]\n",
    "main_df = main_df[(main_df.index < last_5pct)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the function that will pre-process our dataset by normalizing the data and creating sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    # we don't need the future column. we only needed to create the target and we don't want our model to know the future in advance :)\n",
    "    df = df.drop('future', 1)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # target is already a 0/1. we don't need to process it\n",
    "        if col != \"target\":\n",
    "            #normalize the data to show just a % change instead of the actual prices\n",
    "            # remember that the prices are in different scale, bitcoin, eth, ltc, etc.. this way we can have them in the same scale\n",
    "            df[col] = df[col].pct_change()\n",
    "            \n",
    "            # scale the data from -1 to 1 for everything\n",
    "            df[col] = preprocessing.scale(df[col].values)\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    sequential_data = []\n",
    "    # de-que = a list of max items = SEQ_LEN. as the list reachs that size it pops out the old records for us\n",
    "    prev_minutes = deque(maxlen=SEQ_LEN)\n",
    "    \n",
    "    # we need to convert our dataset to a list\n",
    "    for i in df.values:\n",
    "        # this looks confusing, but we're appending to the list all the columns up to -1. \n",
    "        # that is, minus the last column, because we don't want the TARGET\n",
    "        # if our model already knows the target so there's nothing to predict :)\n",
    "        prev_minutes.append([n for n in i[:-1]])\n",
    "        # if we have enough records (whatever SEQ_LEN is let's )\n",
    "        # add to the list and add the label, the Target. after getting the last X minutes of data, what is the target RIGHT now.\n",
    "        if len(prev_minutes) == SEQ_LEN:\n",
    "            sequential_data.append([np.array(prev_minutes), i[-1]])\n",
    "    #shuffle the data\n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    # we need to balance our data so we have the same amount of buys and sell\n",
    "    # if our data is unbalanced that will cause our model to give more weight to one side than the other\n",
    "    buys = []\n",
    "    sells = []\n",
    "    \n",
    "    for seq, target in sequential_data:\n",
    "        if target == 0:\n",
    "            sells.append([seq, target])\n",
    "        elif target == 1:\n",
    "            buys.append([seq, target])\n",
    "    \n",
    "    random.shuffle(buys)\n",
    "    random.shuffle(sells)\n",
    "        \n",
    "    # check which one we have less data\n",
    "    lower = min(len(buys), len(sells))\n",
    "    \n",
    "    # cut the datasets so they are the same size\n",
    "    buys = buys[:lower]\n",
    "    sells = sells[:lower]\n",
    "        \n",
    "    sequential_data = buys+sells\n",
    "    # we don't want our data to be ALL buys and then ALL sells\n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    # we need to break our data into x,y to feed the model\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for seq, target in sequential_data:\n",
    "        x.append(seq)\n",
    "        y.append(target)\n",
    "    \n",
    "    return np.array(x), y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 76380 validation: 3714 \n",
      "Don't buys: 38190, buys: 38190\n",
      "VALIDATION don't buys 1857, buys: 1857\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = preprocess_df(main_df)\n",
    "validation_x, validation_y = preprocess_df(validation_main_df)\n",
    "\n",
    "print(f\"train data: {len(train_x)} validation: {len(validation_x)} \")\n",
    "print(f\"Don't buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
    "print(f\"VALIDATION don't buys {validation_y.count(0)}, buys: {validation_y.count(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/Forex/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/envs/Forex/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 76380 samples, validate on 3714 samples\n",
      "Epoch 1/10\n",
      "76380/76380 [==============================] - 1128s 15ms/sample - loss: 0.7162 - acc: 0.5240 - val_loss: 0.6813 - val_acc: 0.5654\n",
      "Epoch 2/10\n",
      "76380/76380 [==============================] - 1127s 15ms/sample - loss: 0.6847 - acc: 0.5542 - val_loss: 0.6812 - val_acc: 0.5670\n",
      "Epoch 3/10\n",
      "76380/76380 [==============================] - 1114s 15ms/sample - loss: 0.6804 - acc: 0.5664 - val_loss: 0.6794 - val_acc: 0.5716\n",
      "Epoch 4/10\n",
      "76380/76380 [==============================] - 1089s 14ms/sample - loss: 0.6784 - acc: 0.5699 - val_loss: 0.6732 - val_acc: 0.5784\n",
      "Epoch 5/10\n",
      "76380/76380 [==============================] - 1308s 17ms/sample - loss: 0.6762 - acc: 0.5749 - val_loss: 0.6839 - val_acc: 0.5584\n",
      "Epoch 6/10\n",
      "76380/76380 [==============================] - 1268s 17ms/sample - loss: 0.6754 - acc: 0.5774 - val_loss: 0.6821 - val_acc: 0.5681\n",
      "Epoch 7/10\n",
      "76380/76380 [==============================] - 1474s 19ms/sample - loss: 0.6724 - acc: 0.5846 - val_loss: 0.6775 - val_acc: 0.5770\n",
      "Epoch 8/10\n",
      "33664/76380 [============>.................] - ETA: 11:01 - loss: 0.6678 - acc: 0.5944"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    #CuDNNLSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True)\n",
    "    LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True)\n",
    ")\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(\n",
    "    LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True)\n",
    ")\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(\n",
    "    LSTM(128, input_shape=(train_x.shape[1:]))\n",
    ")\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(32, activation=\"tanh\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=opt,\n",
    "             metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=f'logs/{NAME}')\n",
    "\n",
    "filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\" # unique file name that will include the epoch \n",
    "checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_acc',  verbose=1, save_best_only=True, mode='max')) #saves only the best ones\n",
    "\n",
    "history = model.fit(\n",
    "    train_x, train_y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(validation_x, validation_y),\n",
    "    callbacks=[tensorboard, checkpoint]\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
